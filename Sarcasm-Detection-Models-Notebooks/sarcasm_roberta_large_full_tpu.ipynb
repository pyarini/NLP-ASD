{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQJUtH_n38Hj"
      },
      "source": [
        "# Sarcasm Detection (RoBERTa-Large) — Full Colab Notebook (TPU-safe)\n",
        "\n",
        "This notebook is **from scratch** and includes fixes for issues you hit:\n",
        "- Handles datasets (Kaggle SARC + optional HF conversation sarcasm dataset)\n",
        "- Cleans/normalizes labels + context\n",
        "- Stratified train/valid/test split (**ClassLabel** required)\n",
        "- Trains **RoBERTa-Large** on **TPU v5e** using a **TPU-safe PyTorch/XLA loop** (avoids Trainer + fused AdamW issues)\n",
        "- TPU-safe evaluation + classification report\n",
        "- Threshold tuning (overall + hard/subtle subset)\n",
        "- Optional **OpenAI gated ensemble** (only calls LLM when RoBERTa is uncertain; cached + quota-safe)\n",
        "- Saves a final **ensemble package** (model + tokenizer + config + optional LLM cache)\n",
        "\n",
        "> Run on TPU (Runtime → Change runtime type → Hardware accelerator → TPU).  \n",
        "> OpenAI ensemble is optional and requires `OPENAI_API_KEY` with quota.\n"
      ],
      "id": "UQJUtH_n38Hj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PHbjfw438Ho"
      },
      "source": [
        "## 0) Install (TPU-friendly)\n",
        "\n",
        "If you already installed conflicting packages, **restart runtime** after this cell."
      ],
      "id": "-PHbjfw438Ho"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ5nPgJq38Hp",
        "outputId": "19e46d2c-5354-4ace-95e1-de0377a161e3"
      },
      "source": [
        "!pip -q install -U \"datasets>=2.20.0\" \"transformers>=4.40.0\" \"accelerate>=0.30.0\" evaluate scikit-learn kagglehub\n",
        "!pip -q install -U torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html\n",
        "!pip -q install -U openai\n"
      ],
      "id": "lQ5nPgJq38Hp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.1/75.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/512.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.3/512.3 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/8.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m6.3/8.9 MB\u001b[0m \u001b[31m198.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m150.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/69.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/119.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/201.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.0/201.0 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.9/193.9 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.6/221.6 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.8/149.8 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m361.3/361.3 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu-AKJGv38Hr"
      },
      "source": [
        "## 1) Imports + TPU detection"
      ],
      "id": "Mu-AKJGv38Hr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHP48f-C38Hr",
        "outputId": "96c94b8c-1698-4843-ecad-8dc389886cad"
      },
      "source": [
        "import os, re, json, ast, hashlib, time, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from datasets import Dataset, DatasetDict, ClassLabel, concatenate_datasets, load_dataset\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup, set_seed\n",
        "\n",
        "# TPU (PyTorch/XLA)\n",
        "try:\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    import torch_xla.distributed.parallel_loader as pl\n",
        "    TPU_AVAILABLE = True\n",
        "except Exception as e:\n",
        "    TPU_AVAILABLE = False\n",
        "    xm = None\n",
        "    pl = None\n",
        "    print(\"TPU/XLA not available:\", e)\n",
        "\n",
        "SEED = 42\n",
        "set_seed(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "device = xm.xla_device() if TPU_AVAILABLE else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ],
      "id": "rHP48f-C38Hr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: xla:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2979193198.py:27: DeprecationWarning: Use torch_xla.device instead\n",
            "  device = xm.xla_device() if TPU_AVAILABLE else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pLvEqsg38Hs"
      },
      "source": [
        "## 2) Download Reddit SARC from Kaggle (danofer/sarcasm)"
      ],
      "id": "8pLvEqsg38Hs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_UY3XhD38Hs",
        "outputId": "614f83ef-1ffc-4e22-edbd-6a8680bedd9f"
      },
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"danofer/sarcasm\")\n",
        "print(\"Dataset path:\", path)\n"
      ],
      "id": "o_UY3XhD38Hs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'sarcasm' dataset.\n",
            "Dataset path: /kaggle/input/sarcasm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbVt6KaR38Ht"
      },
      "source": [
        "## 3) Load + normalize SARC"
      ],
      "id": "YbVt6KaR38Ht"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "NHtb__BE38Ht",
        "outputId": "a49e8d75-639c-4719-a3b9-7e282dcfa928"
      },
      "source": [
        "import os\n",
        "csv_path = os.path.join(path, \"train-balanced-sarcasm.csv\")\n",
        "sarc = pd.read_csv(csv_path)\n",
        "\n",
        "df_sarc = pd.DataFrame({\n",
        "    \"reply_text\": sarc[\"comment\"].astype(str),\n",
        "    \"context_text\": sarc[\"parent_comment\"].fillna(\"\").astype(str),\n",
        "    \"label\": sarc[\"label\"].astype(int),\n",
        "})\n",
        "\n",
        "print(\"SARC rows:\", len(df_sarc))\n",
        "df_sarc.head()\n"
      ],
      "id": "NHtb__BE38Ht",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SARC rows: 1010826\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          reply_text  \\\n",
              "0                                         NC and NH.   \n",
              "1  You do know west teams play against west teams...   \n",
              "2  They were underdogs earlier today, but since G...   \n",
              "3  This meme isn't funny none of the \"new york ni...   \n",
              "4                    I could use one of those tools.   \n",
              "\n",
              "                                        context_text  label  \n",
              "0  Yeah, I get that argument. At this point, I'd ...      0  \n",
              "1  The blazers and Mavericks (The wests 5 and 6 s...      0  \n",
              "2                            They're favored to win.      0  \n",
              "3                         deadass don't kill my buzz      0  \n",
              "4  Yep can confirm I saw the tool they use for th...      0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-432002b8-a7a6-4f45-ab1b-c697671b9495\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reply_text</th>\n",
              "      <th>context_text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NC and NH.</td>\n",
              "      <td>Yeah, I get that argument. At this point, I'd ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>You do know west teams play against west teams...</td>\n",
              "      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>They were underdogs earlier today, but since G...</td>\n",
              "      <td>They're favored to win.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This meme isn't funny none of the \"new york ni...</td>\n",
              "      <td>deadass don't kill my buzz</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I could use one of those tools.</td>\n",
              "      <td>Yep can confirm I saw the tool they use for th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-432002b8-a7a6-4f45-ab1b-c697671b9495')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-432002b8-a7a6-4f45-ab1b-c697671b9495 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-432002b8-a7a6-4f45-ab1b-c697671b9495');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ebf48339-b589-4ec7-92e9-ec7b4ed9e0af\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ebf48339-b589-4ec7-92e9-ec7b4ed9e0af')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ebf48339-b589-4ec7-92e9-ec7b4ed9e0af button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_sarc"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW_snTl038Hu"
      },
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"danofer/sarcasm\")\n",
        "print(\"Dataset path:\", path)\n",
        "import os\n",
        "csv_path = os.path.join(path, \"train-balanced-sarcasm.csv\")\n",
        "sarc = pd.read_csv(csv_path)\n",
        "\n",
        "df_sarc = pd.DataFrame({\n",
        "    \"reply_text\": sarc[\"comment\"].astype(str),\n",
        "    \"context_text\": sarc[\"parent_comment\"].fillna(\"\").astype(str),\n",
        "    \"label\": sarc[\"label\"].astype(int),\n",
        "})\n",
        "\n",
        "print(\"SARC rows:\", len(df_sarc))\n",
        "df_sarc.head()\n",
        "## 4) (Optional) Load conversation sarcasm dataset from Hugging Face\n",
        "\n",
        "This dataset has columns like `label`, `response`, `context` and labels like `SARCASM`."
      ],
      "id": "vW_snTl038Hu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdVjD91838Hu",
        "outputId": "667693e1-2bfa-4a8b-c807-03f8377b681f"
      },
      "source": [
        "USE_HF_CONV = True  # set False to use only SARC\n",
        "\n",
        "if USE_HF_CONV:\n",
        "    conv_raw = load_dataset(\"shiv213/Automatic-Sarcasm-Detection-Twitter\")[\"train\"]\n",
        "\n",
        "    def normalize_conv(example):\n",
        "        ctx = example.get(\"context\", \"\")\n",
        "        if isinstance(ctx, str) and ctx.strip().startswith(\"[\"):\n",
        "            try:\n",
        "                ctx_list = ast.literal_eval(ctx)\n",
        "                if isinstance(ctx_list, list):\n",
        "                    ctx = \" || \".join([str(x) for x in ctx_list])\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        lbl = example.get(\"label\", 0)\n",
        "        if isinstance(lbl, str):\n",
        "            lbl = 1 if lbl.strip().upper() == \"SARCASM\" else 0\n",
        "        elif isinstance(lbl, bool):\n",
        "            lbl = 1 if lbl else 0\n",
        "        else:\n",
        "            lbl = int(lbl)\n",
        "\n",
        "        return {\"reply_text\": str(example.get(\"response\",\"\")), \"context_text\": str(ctx), \"label\": lbl}\n",
        "\n",
        "    conv = conv_raw.map(normalize_conv, remove_columns=conv_raw.column_names)\n",
        "    print(\"Conv rows:\", len(conv))\n",
        "else:\n",
        "    conv = None\n"
      ],
      "id": "XdVjD91838Hu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conv rows: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxuJ06Kl38Hv"
      },
      "source": [
        "## 5) Combine + cast label to ClassLabel + stratified split"
      ],
      "id": "TxuJ06Kl38Hv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-rl1XHU38Hw",
        "outputId": "844a7fb1-fd63-46f4-f769-260397d29200"
      },
      "source": [
        "SARC_N = 200_000  # lower for faster runs\n",
        "\n",
        "df_sarc_sub = df_sarc.sample(n=min(SARC_N, len(df_sarc)), random_state=SEED).reset_index(drop=True)\n",
        "ds_sarc = Dataset.from_pandas(df_sarc_sub)\n",
        "\n",
        "base = concatenate_datasets([ds_sarc, conv]) if USE_HF_CONV else ds_sarc\n",
        "\n",
        "base = base.cast_column(\"label\", ClassLabel(names=[\"not_sarcasm\", \"sarcasm\"]))\n",
        "\n",
        "split1 = base.train_test_split(test_size=0.20, seed=SEED, stratify_by_column=\"label\")\n",
        "temp = split1[\"test\"].train_test_split(test_size=0.50, seed=SEED, stratify_by_column=\"label\")\n",
        "\n",
        "ds = DatasetDict({\"train\": split1[\"train\"], \"validation\": temp[\"train\"], \"test\": temp[\"test\"]})\n",
        "\n",
        "def sarcasm_pct(d):\n",
        "    y = np.array(d[\"label\"])\n",
        "    return float((y==1).mean()*100)\n",
        "\n",
        "print(\"Train sarcasm %:\", round(sarcasm_pct(ds[\"train\"]),2))\n",
        "print(\"Valid sarcasm %:\", round(sarcasm_pct(ds[\"validation\"]),2))\n",
        "print(\"Test  sarcasm %:\", round(sarcasm_pct(ds[\"test\"]),2))\n"
      ],
      "id": "0-rl1XHU38Hw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train sarcasm %: 49.98\n",
            "Valid sarcasm %: 49.98\n",
            "Test  sarcasm %: 49.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TABT7djf38Hx"
      },
      "source": [
        "## 6) Format multi-turn context + tokenize"
      ],
      "id": "TABT7djf38Hx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452,
          "referenced_widgets": [
            "29a74b5cf4a943e2a3f66ad26626e31a",
            "a1db0df4383e47d0960bce2f8f364e42",
            "718590f69fba4b3bb6f86725ed0121cb",
            "fb00f115af9c4894b49f7279aa878bcc",
            "2a29b7ed9b76480a9aca5c5cc9ea63df",
            "a199e5f48e474b8f858dd7a2ffb3589e",
            "72e88a99df5147cbb3d0999051bbd477",
            "bb6feeea3a374d9c8e3b4350289fcd30",
            "ce62753d0939461b8630aa1122037986",
            "5e636a54762e47f4a433741a2d585b25",
            "58be90f3622c4ec38fdb5122e6d164a1",
            "4c2e2f4f5d4f4bf6bf7fcfa83f627b67",
            "02c02363d5594f6ba67cb2f3480a66b3",
            "02e1bf192bbb4f3ca3f42a9579f579fb",
            "0486272d5102468a85af918e403632bd",
            "0c36dd011d5b4d6b897f627e2957ac05",
            "0aca254b80424a279738375857e84573",
            "b2217ae0bff940ebb15df26fc8d269e1",
            "8dffdfb6d829403ab81cbb93fb3d690d",
            "7f705b5e2bde4372bc1e754db513c352",
            "d4cacdda5b0c4fe097264f5dade3b3bd",
            "ac5513a977094b559dd64e7639cc5722",
            "a82a04917e7c4a3297caa00d6b22d4d3",
            "45ad34e9c62f4ce9ba8610bc49c8f116",
            "c9702b7fc4da41aab8495460ba99ecff",
            "981e3a932c5d43fcb0e405f0bfd48269",
            "a6cd312e484b470883f97bfbcb3e3f4b",
            "d2ee315de85d4d7e8b77d47011bad217",
            "adcdb06504bd4a7d8ab5a490deaec39b",
            "1e1e8b0d67d148659c2a569f56c5f21f",
            "7b742a3ea51d4fd8bd5bbff4bf10e024",
            "29e57d88ab2c45c7ae5626393d989c76",
            "9a7047e0d2494f779125bca25395dfe0",
            "0689a9dd7bef4a72b559bb15848264bc",
            "d1d7c29df2364a0faebd23808f96febe",
            "b695c15fd5cd44348d21194642adeaef",
            "f7a2275284b8458e98767927fd22af13",
            "385426cb9c3c4dde9f9d963a59a64e04",
            "a330624c21b54f6cbac5562ab0b98dd0",
            "fe070bff88404165b975a0d4c699f714",
            "13fa662e9b72429f88f2cb7dae08220a",
            "1156d0f9892a4a9f998632d69bdae418",
            "599fb3cc9911489bbd41ac3dcea7b2a0",
            "1a63484620bc41a3b553080d91bb6a79",
            "2c02d7672ee0487bb8eb6616c37f4942",
            "8ea6b02761b54de79ca5bcc637829621",
            "8648d239b8954a7e90daa273d737453c",
            "7c1d530b74c74675a4e7b0c1d9a7f061",
            "dc0a1b083841413883a8cfea1bffd480",
            "31dbb68567ab484abd42c163852e1dcb",
            "52e2098aa973471fa86ba19520d1a6b3",
            "954420bfccbd4e10ad8ed9d7a07052ea",
            "ca3d01a1df6f421e899a5bb55601921e",
            "0c62ccd27f3d48888ee55b8a487c2e3c",
            "257f859596e7477dbba593c8911852ff",
            "4430d7f2bd2944aeab95f0341124ff00",
            "3ec1e46670e9449a8f05bbc392f02189",
            "71d9fa8a568e49ac9446060802a88ec6",
            "4f69efc2ee74474cba1fc651e50f954e",
            "38ed447565db483eb4d6d62cfee949ca",
            "3ea73ed59edf4eeea34b64501b1b27e3",
            "8e1373903c4146abba2a25eff9a708a8",
            "9284a59e9ddb45eca469fac4ab621070",
            "22714caa36de49acac83871d930b2a98",
            "c6d6653968f743f9a32ad0b8876f7cc6",
            "448d957ed0144129a8cee583376fe321"
          ]
        },
        "id": "qmjxkpKt38Hx",
        "outputId": "3cd28617-fa9f-46a1-b419-ec85ca3398d6"
      },
      "source": [
        "MODEL_NAME = \"roberta-large\"\n",
        "#MAX_LENGTH = 256       # try 384/512 if memory allows\n",
        "#KEEP_LAST_TURNS = 5\n",
        "MAX_LENGTH =512\n",
        "KEEP_LAST_TURNS =7\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def format_input(context_text: str, reply_text: str, keep_last_turns: int = KEEP_LAST_TURNS) -> str:\n",
        "    ctx = (context_text or \"\").strip()\n",
        "    rep = (reply_text or \"\").strip()\n",
        "    turns = [t.strip() for t in ctx.split(\"||\") if t.strip()]\n",
        "    turns = turns[-keep_last_turns:]\n",
        "    if not turns:\n",
        "        ctx_block = \"[NO_CONTEXT]\"\n",
        "    else:\n",
        "        ctx_block = \"\\n\".join([f\"[TURN-{len(turns)-i}] {t}\" for i, t in enumerate(turns)])\n",
        "    return f\"{ctx_block}\\n[REPLY] {rep}\"\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    texts = [format_input(c, r) for c, r in zip(batch[\"context_text\"], batch[\"reply_text\"])]\n",
        "    enc = tokenizer(texts, truncation=True, max_length=MAX_LENGTH, padding=\"max_length\")\n",
        "    enc[\"labels\"] = batch[\"label\"]\n",
        "    return enc\n",
        "\n",
        "encoded = ds.map(tokenize_batch, batched=True, remove_columns=ds[\"train\"].column_names)\n",
        "encoded = encoded.cast_column(\"labels\", ClassLabel(names=[\"not_sarcasm\",\"sarcasm\"]))\n",
        "encoded.set_format(type=\"torch\")\n",
        "encoded\n"
      ],
      "id": "qmjxkpKt38Hx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/164000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29a74b5cf4a943e2a3f66ad26626e31a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/20500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c2e2f4f5d4f4bf6bf7fcfa83f627b67"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/20500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a82a04917e7c4a3297caa00d6b22d4d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Casting the dataset:   0%|          | 0/164000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0689a9dd7bef4a72b559bb15848264bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Casting the dataset:   0%|          | 0/20500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c02d7672ee0487bb8eb6616c37f4942"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Casting the dataset:   0%|          | 0/20500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4430d7f2bd2944aeab95f0341124ff00"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 164000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 20500\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 20500\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pMsIDGp38Hz"
      },
      "source": [
        "## 7) Train RoBERTa-Large with a TPU-safe PyTorch/XLA loop"
      ],
      "id": "8pMsIDGp38Hz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc6jGmu238Hz",
        "outputId": "cc9972b1-aae9-4bde-bc5b-d1f04992cfeb"
      },
      "source": [
        "#EPOCHS = 2\n",
        "#LR = 1e-5\n",
        "WD = 0.01\n",
        "#TRAIN_BS = 16\n",
        "#EVAL_BS = 32\n",
        "#WARMUP_RATIO = 0.06\n",
        "\n",
        "\n",
        "EPOCHS =5\n",
        "LR =1e-5\n",
        "WARMUP_RATIO =0.10\n",
        "TRAIN_BS =16\n",
        "EVAL_BS =32\n",
        "\n",
        "PATIENCE =2\n",
        "best_val_f1 = -1\n",
        "patience_ctr =0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
        "\n",
        "# TPU-safe optimizer\n",
        "if TPU_AVAILABLE:\n",
        "    import torch_xla.amp.syncfree as syncfree\n",
        "    optimizer = syncfree.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "else:\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "train_loader = DataLoader(encoded[\"train\"], batch_size=TRAIN_BS, shuffle=True)\n",
        "valid_loader = DataLoader(encoded[\"validation\"], batch_size=EVAL_BS, shuffle=False)\n",
        "\n",
        "total_steps = EPOCHS * len(train_loader)\n",
        "warmup_steps = int(WARMUP_RATIO * total_steps)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "\n",
        "def run_eval(loader):\n",
        "    model.eval()\n",
        "    all_probs, all_preds, all_labels = [], [], []\n",
        "    it = pl.ParallelLoader(loader, [device]).per_device_loader(device) if TPU_AVAILABLE else loader\n",
        "    with torch.no_grad():\n",
        "        for batch in it:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "            probs = torch.softmax(logits, dim=-1)[:, 1]\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "            all_probs.append(probs.detach().cpu())\n",
        "            all_preds.append(preds.detach().cpu())\n",
        "            all_labels.append(labels.detach().cpu())\n",
        "    p = torch.cat(all_probs).numpy()\n",
        "    yhat = torch.cat(all_preds).numpy()\n",
        "    y = torch.cat(all_labels).numpy()\n",
        "    return y, yhat, p\n",
        "\n",
        "best_val_f1 = -1.0\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    it = pl.ParallelLoader(train_loader, [device]).per_device_loader(device) if TPU_AVAILABLE else train_loader\n",
        "\n",
        "    for step, batch in enumerate(it):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = out.loss\n",
        "        loss.backward()\n",
        "\n",
        "        if TPU_AVAILABLE:\n",
        "            xm.optimizer_step(optimizer)\n",
        "        else:\n",
        "            optimizer.step()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scheduler.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if (step + 1) % 200 == 0:\n",
        "            msg = f\"Epoch {epoch+1} step {step+1}/{len(train_loader)} loss {running_loss/(step+1):.4f}\"\n",
        "            if TPU_AVAILABLE: xm.master_print(msg)\n",
        "            else: print(msg)\n",
        "\n",
        "    yv, yvhat, _ = run_eval(valid_loader)\n",
        "    val_f1 = f1_score(yv, yvhat)\n",
        "    msg = f\"Epoch {epoch+1}: train_loss={running_loss/len(train_loader):.4f} val_f1={val_f1:.4f}\"\n",
        "    if TPU_AVAILABLE: xm.master_print(msg)\n",
        "    else: print(msg)\n",
        "\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "        #best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "        patience_ctr =0\n",
        "    else:\n",
        "        patience_ctr +=1\n",
        "\n",
        "    if patience_ctr >= PATIENCE:\n",
        "      print(\"Early stopping\")\n",
        "      break\n",
        "\n",
        "\n",
        "\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "\n",
        "print(\"Best val F1:\", best_val_f1)\n"
      ],
      "id": "cc6jGmu238Hz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 step 200/10250 loss 0.6999\n",
            "Epoch 1 step 400/10250 loss 0.6975\n",
            "Epoch 1 step 600/10250 loss 0.6965\n",
            "Epoch 1 step 800/10250 loss 0.6923\n",
            "Epoch 1 step 1000/10250 loss 0.6815\n",
            "Epoch 1 step 1200/10250 loss 0.6692\n",
            "Epoch 1 step 1400/10250 loss 0.6576\n",
            "Epoch 1 step 1600/10250 loss 0.6451\n",
            "Epoch 1 step 1800/10250 loss 0.6370\n",
            "Epoch 1 step 2000/10250 loss 0.6282\n",
            "Epoch 1 step 2200/10250 loss 0.6199\n",
            "Epoch 1 step 2400/10250 loss 0.6137\n",
            "Epoch 1 step 2600/10250 loss 0.6087\n",
            "Epoch 1 step 2800/10250 loss 0.6031\n",
            "Epoch 1 step 3000/10250 loss 0.5976\n",
            "Epoch 1 step 3200/10250 loss 0.5944\n",
            "Epoch 1 step 3400/10250 loss 0.5891\n",
            "Epoch 1 step 3600/10250 loss 0.5850\n",
            "Epoch 1 step 3800/10250 loss 0.5819\n",
            "Epoch 1 step 4000/10250 loss 0.5779\n",
            "Epoch 1 step 4200/10250 loss 0.5747\n",
            "Epoch 1 step 4400/10250 loss 0.5727\n",
            "Epoch 1 step 4600/10250 loss 0.5704\n",
            "Epoch 1 step 4800/10250 loss 0.5686\n",
            "Epoch 1 step 5000/10250 loss 0.5667\n",
            "Epoch 1 step 5200/10250 loss 0.5650\n",
            "Epoch 1 step 5400/10250 loss 0.5631\n",
            "Epoch 1 step 5600/10250 loss 0.5609\n",
            "Epoch 1 step 5800/10250 loss 0.5594\n",
            "Epoch 1 step 6000/10250 loss 0.5573\n",
            "Epoch 1 step 6200/10250 loss 0.5556\n",
            "Epoch 1 step 6400/10250 loss 0.5541\n",
            "Epoch 1 step 6600/10250 loss 0.5524\n",
            "Epoch 1 step 6800/10250 loss 0.5504\n",
            "Epoch 1 step 7000/10250 loss 0.5493\n",
            "Epoch 1 step 7200/10250 loss 0.5480\n",
            "Epoch 1 step 7400/10250 loss 0.5466\n",
            "Epoch 1 step 7600/10250 loss 0.5452\n",
            "Epoch 1 step 7800/10250 loss 0.5438\n",
            "Epoch 1 step 8000/10250 loss 0.5422\n",
            "Epoch 1 step 8200/10250 loss 0.5410\n",
            "Epoch 1 step 8400/10250 loss 0.5396\n",
            "Epoch 1 step 8600/10250 loss 0.5384\n",
            "Epoch 1 step 8800/10250 loss 0.5376\n",
            "Epoch 1 step 9000/10250 loss 0.5365\n",
            "Epoch 1 step 9200/10250 loss 0.5359\n",
            "Epoch 1 step 9400/10250 loss 0.5347\n",
            "Epoch 1 step 9600/10250 loss 0.5334\n",
            "Epoch 1 step 9800/10250 loss 0.5325\n",
            "Epoch 1 step 10000/10250 loss 0.5314\n",
            "Epoch 1 step 10200/10250 loss 0.5304\n",
            "Epoch 1: train_loss=0.5303 val_f1=0.7766\n",
            "Epoch 2 step 200/10250 loss 0.4253\n",
            "Epoch 2 step 400/10250 loss 0.4368\n",
            "Epoch 2 step 600/10250 loss 0.4331\n",
            "Epoch 2 step 800/10250 loss 0.4313\n",
            "Epoch 2 step 1000/10250 loss 0.4356\n",
            "Epoch 2 step 1200/10250 loss 0.4351\n",
            "Epoch 2 step 1400/10250 loss 0.4323\n",
            "Epoch 2 step 1600/10250 loss 0.4294\n",
            "Epoch 2 step 1800/10250 loss 0.4289\n",
            "Epoch 2 step 2000/10250 loss 0.4297\n",
            "Epoch 2 step 2200/10250 loss 0.4298\n",
            "Epoch 2 step 2400/10250 loss 0.4301\n",
            "Epoch 2 step 2600/10250 loss 0.4306\n",
            "Epoch 2 step 2800/10250 loss 0.4300\n",
            "Epoch 2 step 3000/10250 loss 0.4289\n",
            "Epoch 2 step 3200/10250 loss 0.4281\n",
            "Epoch 2 step 3400/10250 loss 0.4289\n",
            "Epoch 2 step 3600/10250 loss 0.4297\n",
            "Epoch 2 step 3800/10250 loss 0.4288\n",
            "Epoch 2 step 4000/10250 loss 0.4290\n",
            "Epoch 2 step 4200/10250 loss 0.4282\n",
            "Epoch 2 step 4400/10250 loss 0.4286\n",
            "Epoch 2 step 4600/10250 loss 0.4283\n",
            "Epoch 2 step 4800/10250 loss 0.4278\n",
            "Epoch 2 step 5000/10250 loss 0.4279\n",
            "Epoch 2 step 5200/10250 loss 0.4283\n",
            "Epoch 2 step 5400/10250 loss 0.4284\n",
            "Epoch 2 step 5600/10250 loss 0.4280\n",
            "Epoch 2 step 5800/10250 loss 0.4281\n",
            "Epoch 2 step 6000/10250 loss 0.4278\n",
            "Epoch 2 step 6200/10250 loss 0.4278\n",
            "Epoch 2 step 6400/10250 loss 0.4278\n",
            "Epoch 2 step 6600/10250 loss 0.4272\n",
            "Epoch 2 step 6800/10250 loss 0.4269\n",
            "Epoch 2 step 7000/10250 loss 0.4268\n",
            "Epoch 2 step 7200/10250 loss 0.4267\n",
            "Epoch 2 step 7400/10250 loss 0.4267\n",
            "Epoch 2 step 7600/10250 loss 0.4266\n",
            "Epoch 2 step 7800/10250 loss 0.4264\n",
            "Epoch 2 step 8000/10250 loss 0.4271\n",
            "Epoch 2 step 8200/10250 loss 0.4273\n",
            "Epoch 2 step 8400/10250 loss 0.4273\n",
            "Epoch 2 step 8600/10250 loss 0.4271\n",
            "Epoch 2 step 8800/10250 loss 0.4269\n",
            "Epoch 2 step 9000/10250 loss 0.4268\n",
            "Epoch 2 step 9200/10250 loss 0.4268\n",
            "Epoch 2 step 9400/10250 loss 0.4270\n",
            "Epoch 2 step 9600/10250 loss 0.4274\n",
            "Epoch 2 step 9800/10250 loss 0.4270\n",
            "Epoch 2 step 10000/10250 loss 0.4273\n",
            "Epoch 2 step 10200/10250 loss 0.4275\n",
            "Epoch 2: train_loss=0.4275 val_f1=0.7824\n",
            "Epoch 3 step 200/10250 loss 0.3299\n",
            "Epoch 3 step 400/10250 loss 0.3268\n",
            "Epoch 3 step 600/10250 loss 0.3274\n",
            "Epoch 3 step 800/10250 loss 0.3279\n",
            "Epoch 3 step 1000/10250 loss 0.3249\n",
            "Epoch 3 step 1200/10250 loss 0.3244\n",
            "Epoch 3 step 1400/10250 loss 0.3247\n",
            "Epoch 3 step 1600/10250 loss 0.3226\n",
            "Epoch 3 step 1800/10250 loss 0.3221\n",
            "Epoch 3 step 2000/10250 loss 0.3217\n",
            "Epoch 3 step 2200/10250 loss 0.3238\n",
            "Epoch 3 step 2400/10250 loss 0.3240\n",
            "Epoch 3 step 2600/10250 loss 0.3242\n",
            "Epoch 3 step 2800/10250 loss 0.3242\n",
            "Epoch 3 step 3000/10250 loss 0.3246\n",
            "Epoch 3 step 3200/10250 loss 0.3251\n",
            "Epoch 3 step 3400/10250 loss 0.3257\n",
            "Epoch 3 step 3600/10250 loss 0.3263\n",
            "Epoch 3 step 3800/10250 loss 0.3267\n",
            "Epoch 3 step 4000/10250 loss 0.3263\n",
            "Epoch 3 step 4200/10250 loss 0.3264\n",
            "Epoch 3 step 4400/10250 loss 0.3270\n",
            "Epoch 3 step 4600/10250 loss 0.3269\n",
            "Epoch 3 step 4800/10250 loss 0.3269\n",
            "Epoch 3 step 5000/10250 loss 0.3269\n",
            "Epoch 3 step 5200/10250 loss 0.3263\n",
            "Epoch 3 step 5400/10250 loss 0.3268\n",
            "Epoch 3 step 5600/10250 loss 0.3267\n",
            "Epoch 3 step 5800/10250 loss 0.3272\n",
            "Epoch 3 step 6000/10250 loss 0.3271\n",
            "Epoch 3 step 6200/10250 loss 0.3269\n",
            "Epoch 3 step 6400/10250 loss 0.3272\n",
            "Epoch 3 step 6600/10250 loss 0.3270\n",
            "Epoch 3 step 6800/10250 loss 0.3268\n",
            "Epoch 3 step 7000/10250 loss 0.3269\n",
            "Epoch 3 step 7200/10250 loss 0.3270\n",
            "Epoch 3 step 7400/10250 loss 0.3262\n",
            "Epoch 3 step 7600/10250 loss 0.3264\n",
            "Epoch 3 step 7800/10250 loss 0.3263\n",
            "Epoch 3 step 8000/10250 loss 0.3263\n",
            "Epoch 3 step 8200/10250 loss 0.3268\n",
            "Epoch 3 step 8400/10250 loss 0.3267\n",
            "Epoch 3 step 8600/10250 loss 0.3265\n",
            "Epoch 3 step 8800/10250 loss 0.3265\n",
            "Epoch 3 step 9000/10250 loss 0.3268\n",
            "Epoch 3 step 9200/10250 loss 0.3267\n",
            "Epoch 3 step 9400/10250 loss 0.3265\n",
            "Epoch 3 step 9600/10250 loss 0.3262\n",
            "Epoch 3 step 9800/10250 loss 0.3263\n",
            "Epoch 3 step 10000/10250 loss 0.3258\n",
            "Epoch 3 step 10200/10250 loss 0.3258\n",
            "Epoch 3: train_loss=0.3256 val_f1=0.7914\n",
            "Epoch 4 step 200/10250 loss 0.2353\n",
            "Epoch 4 step 400/10250 loss 0.2232\n",
            "Epoch 4 step 600/10250 loss 0.2242\n",
            "Epoch 4 step 800/10250 loss 0.2233\n",
            "Epoch 4 step 1000/10250 loss 0.2218\n",
            "Epoch 4 step 1200/10250 loss 0.2211\n",
            "Epoch 4 step 1400/10250 loss 0.2209\n",
            "Epoch 4 step 1600/10250 loss 0.2206\n",
            "Epoch 4 step 1800/10250 loss 0.2215\n",
            "Epoch 4 step 2000/10250 loss 0.2223\n",
            "Epoch 4 step 2200/10250 loss 0.2229\n",
            "Epoch 4 step 2400/10250 loss 0.2228\n",
            "Epoch 4 step 2600/10250 loss 0.2228\n",
            "Epoch 4 step 2800/10250 loss 0.2235\n",
            "Epoch 4 step 3000/10250 loss 0.2243\n",
            "Epoch 4 step 3200/10250 loss 0.2250\n",
            "Epoch 4 step 3400/10250 loss 0.2246\n",
            "Epoch 4 step 3600/10250 loss 0.2245\n",
            "Epoch 4 step 3800/10250 loss 0.2238\n",
            "Epoch 4 step 4000/10250 loss 0.2244\n",
            "Epoch 4 step 4200/10250 loss 0.2244\n",
            "Epoch 4 step 4400/10250 loss 0.2250\n",
            "Epoch 4 step 4600/10250 loss 0.2253\n",
            "Epoch 4 step 4800/10250 loss 0.2258\n",
            "Epoch 4 step 5000/10250 loss 0.2257\n",
            "Epoch 4 step 5200/10250 loss 0.2262\n",
            "Epoch 4 step 5400/10250 loss 0.2262\n",
            "Epoch 4 step 5600/10250 loss 0.2266\n",
            "Epoch 4 step 5800/10250 loss 0.2261\n",
            "Epoch 4 step 6000/10250 loss 0.2258\n",
            "Epoch 4 step 6200/10250 loss 0.2253\n",
            "Epoch 4 step 6400/10250 loss 0.2255\n",
            "Epoch 4 step 6600/10250 loss 0.2255\n",
            "Epoch 4 step 6800/10250 loss 0.2249\n",
            "Epoch 4 step 7000/10250 loss 0.2246\n",
            "Epoch 4 step 7200/10250 loss 0.2250\n",
            "Epoch 4 step 7400/10250 loss 0.2250\n",
            "Epoch 4 step 7600/10250 loss 0.2250\n",
            "Epoch 4 step 7800/10250 loss 0.2247\n",
            "Epoch 4 step 8000/10250 loss 0.2248\n",
            "Epoch 4 step 8200/10250 loss 0.2245\n",
            "Epoch 4 step 8400/10250 loss 0.2245\n",
            "Epoch 4 step 8600/10250 loss 0.2243\n",
            "Epoch 4 step 8800/10250 loss 0.2239\n",
            "Epoch 4 step 9000/10250 loss 0.2237\n",
            "Epoch 4 step 9200/10250 loss 0.2239\n",
            "Epoch 4 step 9400/10250 loss 0.2238\n",
            "Epoch 4 step 9600/10250 loss 0.2238\n",
            "Epoch 4 step 9800/10250 loss 0.2236\n",
            "Epoch 4 step 10000/10250 loss 0.2236\n",
            "Epoch 4 step 10200/10250 loss 0.2236\n",
            "Epoch 4: train_loss=0.2236 val_f1=0.7936\n",
            "Epoch 5 step 200/10250 loss 0.1463\n",
            "Epoch 5 step 400/10250 loss 0.1508\n",
            "Epoch 5 step 600/10250 loss 0.1498\n",
            "Epoch 5 step 800/10250 loss 0.1508\n",
            "Epoch 5 step 1000/10250 loss 0.1498\n",
            "Epoch 5 step 1200/10250 loss 0.1490\n",
            "Epoch 5 step 1400/10250 loss 0.1507\n",
            "Epoch 5 step 1600/10250 loss 0.1508\n",
            "Epoch 5 step 1800/10250 loss 0.1507\n",
            "Epoch 5 step 2000/10250 loss 0.1504\n",
            "Epoch 5 step 2200/10250 loss 0.1503\n",
            "Epoch 5 step 2400/10250 loss 0.1500\n",
            "Epoch 5 step 2600/10250 loss 0.1499\n",
            "Epoch 5 step 2800/10250 loss 0.1494\n",
            "Epoch 5 step 3000/10250 loss 0.1491\n",
            "Epoch 5 step 3200/10250 loss 0.1493\n",
            "Epoch 5 step 3400/10250 loss 0.1493\n",
            "Epoch 5 step 3600/10250 loss 0.1490\n",
            "Epoch 5 step 3800/10250 loss 0.1483\n",
            "Epoch 5 step 4000/10250 loss 0.1481\n",
            "Epoch 5 step 4200/10250 loss 0.1486\n",
            "Epoch 5 step 4400/10250 loss 0.1480\n",
            "Epoch 5 step 4600/10250 loss 0.1483\n",
            "Epoch 5 step 4800/10250 loss 0.1480\n",
            "Epoch 5 step 5000/10250 loss 0.1489\n",
            "Epoch 5 step 5200/10250 loss 0.1484\n",
            "Epoch 5 step 5400/10250 loss 0.1483\n",
            "Epoch 5 step 5600/10250 loss 0.1480\n",
            "Epoch 5 step 5800/10250 loss 0.1479\n",
            "Epoch 5 step 6000/10250 loss 0.1481\n",
            "Epoch 5 step 6200/10250 loss 0.1480\n",
            "Epoch 5 step 6400/10250 loss 0.1481\n",
            "Epoch 5 step 6600/10250 loss 0.1484\n",
            "Epoch 5 step 6800/10250 loss 0.1482\n",
            "Epoch 5 step 7000/10250 loss 0.1480\n",
            "Epoch 5 step 7200/10250 loss 0.1484\n",
            "Epoch 5 step 7400/10250 loss 0.1478\n",
            "Epoch 5 step 7600/10250 loss 0.1479\n",
            "Epoch 5 step 7800/10250 loss 0.1477\n",
            "Epoch 5 step 8000/10250 loss 0.1478\n",
            "Epoch 5 step 8200/10250 loss 0.1475\n",
            "Epoch 5 step 8400/10250 loss 0.1475\n",
            "Epoch 5 step 8600/10250 loss 0.1476\n",
            "Epoch 5 step 8800/10250 loss 0.1475\n",
            "Epoch 5 step 9000/10250 loss 0.1472\n",
            "Epoch 5 step 9200/10250 loss 0.1474\n",
            "Epoch 5 step 9400/10250 loss 0.1470\n",
            "Epoch 5 step 9600/10250 loss 0.1469\n",
            "Epoch 5 step 9800/10250 loss 0.1466\n",
            "Epoch 5 step 10000/10250 loss 0.1465\n",
            "Epoch 5 step 10200/10250 loss 0.1464\n",
            "Epoch 5: train_loss=0.1464 val_f1=0.7891\n",
            "Best val F1: 0.7936082671514687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmReEX_k38H0"
      },
      "source": [
        "## 8) Test evaluation (TPU-safe) + report"
      ],
      "id": "mmReEX_k38H0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxRGC6Go38H0",
        "outputId": "3655b9d1-5e6b-4e14-ebb8-f886beb73859"
      },
      "source": [
        "test_loader = DataLoader(encoded[\"test\"], batch_size=EVAL_BS, shuffle=False)\n",
        "y_test, y_pred, p_test = run_eval(test_loader)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, target_names=[\"not_sarcasm\",\"sarcasm\"]))\n"
      ],
      "id": "vxRGC6Go38H0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[7857 2396]\n",
            " [1944 8303]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " not_sarcasm       0.80      0.77      0.78     10253\n",
            "     sarcasm       0.78      0.81      0.79     10247\n",
            "\n",
            "    accuracy                           0.79     20500\n",
            "   macro avg       0.79      0.79      0.79     20500\n",
            "weighted avg       0.79      0.79      0.79     20500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_DIR = \"sarcasm_roberta_large_context\"\n",
        "model.save_model(SAVE_DIR)\n",
        "tokenizer.save_pretrained(SAVE_DIR)\n",
        "\n",
        "import shutil\n",
        "shutil.make_archive(SAVE_DIR, \"zip\", SAVE_DIR)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(f\"{SAVE_DIR}.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "4JDl2c7wYP-a",
        "outputId": "e5403695-ad8e-42e2-cdbb-65d4237b66d2"
      },
      "id": "4JDl2c7wYP-a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'RobertaForSequenceClassification' object has no attribute 'save_model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2433322612.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mSAVE_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sarcasm_roberta_large_context\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1962\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1965\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'RobertaForSequenceClassification' object has no attribute 'save_model'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7pcLhqsiP1A",
        "outputId": "b51108d3-7d49-4304-97de-35ea0f541f8c"
      },
      "id": "B7pcLhqsiP1A",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpHiOeLd38H1"
      },
      "source": [
        "## 9) Threshold tuning (overall + hard/subtle subset)"
      ],
      "id": "rpHiOeLd38H1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jypVx3C38H1",
        "outputId": "22088d85-0bb2-4c86-db78-6f8452013d6a"
      },
      "source": [
        "yv, _, pv = run_eval(valid_loader)\n",
        "\n",
        "best_t, best_f = 0.5, 0.0\n",
        "for t in np.linspace(0.1, 0.9, 81):\n",
        "    yhat = (pv >= t).astype(int)\n",
        "    f = f1_score(yv, yhat)\n",
        "    if f > best_f:\n",
        "        best_f, best_t = f, t\n",
        "print(\"Best threshold (val overall):\", best_t, \"F1:\", best_f)\n",
        "\n",
        "unc = np.abs(pv - 0.5)\n",
        "hard_ids = np.argsort(unc)[:1000]\n",
        "yv_h, pv_h = yv[hard_ids], pv[hard_ids]\n",
        "\n",
        "best_t_h, best_f_h = 0.5, 0.0\n",
        "for t in np.linspace(0.1, 0.9, 81):\n",
        "    yhat = (pv_h >= t).astype(int)\n",
        "    f = f1_score(yv_h, yhat)\n",
        "    if f > best_f_h:\n",
        "        best_f_h, best_t_h = f, t\n",
        "print(\"Best threshold (val HARD):\", best_t_h, \"Hard F1:\", best_f_h)\n",
        "\n",
        "FINAL_THRESHOLD = float(best_t_h)  # choose best_t for overall, best_t_h for subtle\n",
        "y_pred_thr = (p_test >= FINAL_THRESHOLD).astype(int)\n",
        "\n",
        "print(\"\\nTEST @ FINAL_THRESHOLD =\", FINAL_THRESHOLD)\n",
        "print(confusion_matrix(y_test, y_pred_thr))\n",
        "print(classification_report(y_test, y_pred_thr, target_names=[\"not_sarcasm\",\"sarcasm\"]))\n"
      ],
      "id": "7jypVx3C38H1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best threshold (val overall): 0.37 F1: 0.7952297309790145\n",
            "Best threshold (val HARD): 0.1 Hard F1: 0.6225895316804407\n",
            "\n",
            "TEST @ FINAL_THRESHOLD = 0.1\n",
            "[[6097 4156]\n",
            " [ 975 9272]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " not_sarcasm       0.86      0.59      0.70     10253\n",
            "     sarcasm       0.69      0.90      0.78     10247\n",
            "\n",
            "    accuracy                           0.75     20500\n",
            "   macro avg       0.78      0.75      0.74     20500\n",
            "weighted avg       0.78      0.75      0.74     20500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fFaOxzo38H3"
      },
      "source": [
        "## 10) Optional OpenAI gated ensemble (only calls LLM when RoBERTa uncertain)\n",
        "\n",
        "Set `USE_OPENAI_ENSEMBLE=True` and ensure you have API quota."
      ],
      "id": "8fFaOxzo38H3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtfTw69n38H3",
        "outputId": "9d570e12-9d60-40c3-af7e-ae5456b8e171"
      },
      "source": [
        "USE_OPENAI_ENSEMBLE = True\n",
        "UNCERTAINTY_GATE = 0.08\n",
        "LLM_MODEL = \"gpt-4o-mini\"\n",
        "LLM_CONF_GATE = 0.88\n",
        "W_ROBERTA_CONF, W_LLM_CONF = 0.75, 0.25\n",
        "W_ROBERTA_BASE, W_LLM_BASE = 0.90, 0.10\n",
        "FINAL_THRESHOLD = 0.56\n",
        "_llm_cache = {}\n",
        "\n",
        "if USE_OPENAI_ENSEMBLE:\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI()\n",
        "\n",
        "    SYSTEM_PROMPT = (\n",
        "        \"You are a careful sarcasm detector for social media conversations.\\n\"\n",
        "        \"Output ONLY JSON: \"\n",
        "        '{\"label\":\"SARCASTIC\"|\"NOT_SARCASTIC\",\"confidence\":0-1}.'\n",
        "    )\n",
        "\n",
        "    def _cache_key(ctx, rep):\n",
        "        s = format_input(ctx, rep)\n",
        "        return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "    def llm_judge(ctx, rep, model_name=LLM_MODEL):\n",
        "        k = _cache_key(ctx, rep)\n",
        "        if k in _llm_cache:\n",
        "            return _llm_cache[k]\n",
        "\n",
        "        prompt = (\n",
        "            \"Decide whether the [REPLY] is sarcastic given the context.\\n\\n\"\n",
        "            f\"{format_input(ctx, rep)}\\n\\nReturn JSON only.\"\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            resp = client.responses.create(\n",
        "                model=model_name,\n",
        "                input=[\n",
        "                    {\"role\":\"developer\",\"content\":SYSTEM_PROMPT},\n",
        "                    {\"role\":\"user\",\"content\":prompt},\n",
        "                ],\n",
        "                temperature=0.0,\n",
        "            )\n",
        "            text = (resp.output_text or \"\").strip()\n",
        "            m = re.search(r\"\\{.*\\}\", text, flags=re.S)\n",
        "            if not m:\n",
        "                out = (0, 0.5)\n",
        "            else:\n",
        "                obj = json.loads(m.group(0))\n",
        "                lab = str(obj.get(\"label\",\"\")).upper()\n",
        "                conf = float(obj.get(\"confidence\",0.5))\n",
        "                conf = max(0.0, min(1.0, conf))\n",
        "                out = (1 if lab==\"SARCASTIC\" else 0, conf)\n",
        "        except Exception:\n",
        "            out = (0, 0.5)\n",
        "\n",
        "        _llm_cache[k] = out\n",
        "        time.sleep(0.2)\n",
        "        return out\n",
        "\n",
        "    #def roberta_prob_one(ctx, rep):\n",
        "     #   text = format_input(ctx, rep)\n",
        "     #   inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH)\n",
        "     #   inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "     #   model.eval()\n",
        "     #   with torch.no_grad():\n",
        "      #      logits = model(**inputs).logits\n",
        "      #      p = torch.softmax(logits, dim=-1)[0,1].detach().cpu().item()\n",
        "      #  return float(p)\n",
        "\n",
        "    def ensemble_predict_one(ctx, rep):\n",
        "        p_r = roberta_prob_one(ctx, rep, max_length=MAX_LENGTH)\n",
        "        if abs(p_r - 0.5) >= UNCERTAINTY_GATE:\n",
        "            pred = 1 if p_r >= FINAL_THRESHOLD else 0\n",
        "            return pred, {\"used_llm\": False, \"p_roberta\": p_r, \"p_final\": p_r}\n",
        "\n",
        "        llm_label, llm_conf = llm_judge(ctx, rep)\n",
        "        p_l = llm_conf if llm_label==1 else (1.0-llm_conf)\n",
        "\n",
        "        if llm_conf >= LLM_CONF_GATE:\n",
        "            w_r, w_l = W_ROBERTA_CONF, W_LLM_CONF\n",
        "        else:\n",
        "            w_r, w_l = W_ROBERTA_BASE, W_LLM_BASE\n",
        "\n",
        "        p_final = w_r*p_r + w_l*p_l\n",
        "        pred = 1 if p_final >= FINAL_THRESHOLD else 0\n",
        "        return pred, {\"used_llm\": True, \"p_roberta\": p_r, \"p_llm\": p_l, \"p_final\": p_final, \"llm_conf\": llm_conf}\n",
        "\n",
        "    print(\"OpenAI ensemble enabled.\")\n",
        "else:\n",
        "    print(\"OpenAI ensemble disabled.\")\n"
      ],
      "id": "TtfTw69n38H3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI ensemble enabled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TPU-safe device getter\n",
        "try:\n",
        "    import torch_xla\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    TPU_AVAILABLE = True\n",
        "    DEVICE = torch_xla.device()   # recommended (no deprecation warning)\n",
        "except Exception:\n",
        "    TPU_AVAILABLE = False\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "def roberta_prob_one(ctx, rep, max_length=256):\n",
        "    text = format_input(ctx, rep)\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=False,\n",
        "    )\n",
        "\n",
        "    # IMPORTANT: move ALL tensors to DEVICE (XLA if TPU)\n",
        "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        p = torch.softmax(logits, dim=-1)[0, 1].detach().cpu().item()\n",
        "    return float(p)\n"
      ],
      "metadata": {
        "id": "6Im9zb76mZTu"
      },
      "id": "6Im9zb76mZTu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcvb-A0h38H4"
      },
      "source": [
        "## 11) Save final package (RoBERTa-Large + config + optional LLM cache)"
      ],
      "id": "kcvb-A0h38H4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "B6otbJ0D38H4",
        "outputId": "92ddafc7-d938-4615-dc4c-880b41f070c3"
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "SAVE_DIR = \"sarcasm_roberta_large_package\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Save model/tokenizer to CPU-friendly format\n",
        "model_cpu = model.to(\"cpu\")\n",
        "model_cpu.save_pretrained(SAVE_DIR)\n",
        "tokenizer.save_pretrained(SAVE_DIR)\n",
        "\n",
        "package_config = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"max_length\": int(MAX_LENGTH),\n",
        "    \"keep_last_turns\": int(KEEP_LAST_TURNS),\n",
        "    \"final_threshold\": float(FINAL_THRESHOLD),\n",
        "    \"use_openai_ensemble\": bool(USE_OPENAI_ENSEMBLE),\n",
        "    \"uncertainty_gate\": float(UNCERTAINTY_GATE),\n",
        "    \"llm_model\": LLM_MODEL,\n",
        "    \"llm_conf_gate\": float(LLM_CONF_GATE),\n",
        "    \"weights\": {\n",
        "        \"roberta_conf\": float(W_ROBERTA_CONF),\n",
        "        \"llm_conf\": float(W_LLM_CONF),\n",
        "        \"roberta_base\": float(W_ROBERTA_BASE),\n",
        "        \"llm_base\": float(W_LLM_BASE),\n",
        "    },\n",
        "}\n",
        "\n",
        "with open(os.path.join(SAVE_DIR, \"ensemble_config.json\"), \"w\") as f:\n",
        "    json.dump(package_config, f, indent=2)\n",
        "\n",
        "# Check if _llm_cache is defined and save it\n",
        "if '_llm_cache' in locals() or '_llm_cache' in globals():\n",
        "    with open(os.path.join(SAVE_DIR, \"llm_cache.json\"), \"w\") as f:\n",
        "        json.dump(_llm_cache, f)\n",
        "else:\n",
        "    print(\"Warning: _llm_cache not found. Skipping saving LLM cache.\")\n",
        "\n",
        "shutil.make_archive(SAVE_DIR, \"zip\", SAVE_DIR)\n",
        "print(\"Saved:\", f\"{SAVE_DIR}.zip\")\n",
        "# Check size\n",
        "p = os.path.join(SAVE_DIR, \"model.safetensors\")\n",
        "print(\"New size (GB):\", os.path.getsize(p)/1024**3)\n",
        "print(\"Done ✅\")\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(f\"{SAVE_DIR}.zip\")\n",
        "except Exception as e:\n",
        "    print(\"Download helper not available:\", e)\n"
      ],
      "id": "B6otbJ0D38H4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: sarcasm_roberta_large_package.zip\n",
            "New size (GB): 1.3238707706332207\n",
            "Done ✅\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3f50c36a-712b-47d1-8b16-436865d7a208\", \"sarcasm_roberta_large_package.zip\", 1317284251)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model = model.to(device) # Ensure the model is on the correct device for XLA/TPU operations\n",
        "\n",
        "ctx = \"I waited 2 hours for this. || They said it would be quick.\"\n",
        "rep = \"Wow, so efficient.\"\n",
        "\n",
        "pred, dbg = ensemble_predict_one(ctx, rep)\n",
        "print(\"Pred:\", \"sarcasm\" if pred==1 else \"not_sarcasm\")\n",
        "print(\"Debug:\", dbg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "DXIcmhFOjR6M",
        "outputId": "ecf11a6b-82d3-406b-f030-af13760a3e8d"
      },
      "id": "DXIcmhFOjR6M",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected XLA tensor. Got: torch.FloatTensor",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2115294945.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Wow, so efficient.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble_predict_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pred:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sarcasm\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"not_sarcasm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Debug:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-766154328.py\u001b[0m in \u001b[0;36mensemble_predict_one\u001b[0;34m(ctx, rep)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mensemble_predict_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mp_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroberta_prob_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_r\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mUNCERTAINTY_GATE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp_r\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mFINAL_THRESHOLD\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3659846497.py\u001b[0m in \u001b[0;36mroberta_prob_one\u001b[0;34m(ctx, rep, max_length)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m   1189\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    796\u001b[0m                 \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    799\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2540\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2542\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected XLA tensor. Got: torch.FloatTensor"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "FINAL_THRESHOLD = 0.56\n",
        "\n",
        "def eval_ensemble_on_test(N=500):  # start with 200–1000 to control cost\n",
        "    y_true, y_pred = [], []\n",
        "    used_llm = 0\n",
        "\n",
        "    for i in range(min(N, len(ds[\"test\"]))):\n",
        "        ex = ds[\"test\"][i]\n",
        "        pred, dbg = ensemble_predict_one(ex[\"context_text\"], ex[\"reply_text\"])\n",
        "        y_true.append(int(ex[\"label\"]))\n",
        "        y_pred.append(int(pred))\n",
        "        if dbg.get(\"used_llm\"):\n",
        "            used_llm += 1\n",
        "\n",
        "    print(\"Evaluated:\", len(y_true))\n",
        "    print(\"LLM used for:\", used_llm, \"examples (\", round(100*used_llm/len(y_true),2), \"% )\")\n",
        "\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred, target_names=[\"not_sarcasm\",\"sarcasm\"]))\n",
        "\n",
        "# Run\n",
        "eval_ensemble_on_test(N=500)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "BBIXdyS7jTAS",
        "outputId": "14a7847c-7fef-445a-9235-b9eacea467a1"
      },
      "id": "BBIXdyS7jTAS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected XLA tensor. Got: torch.FloatTensor",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2612821703.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0meval_ensemble_on_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2612821703.py\u001b[0m in \u001b[0;36meval_ensemble_on_test\u001b[0;34m(N)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble_predict_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reply_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-766154328.py\u001b[0m in \u001b[0;36mensemble_predict_one\u001b[0;34m(ctx, rep)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mensemble_predict_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mp_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroberta_prob_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_r\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mUNCERTAINTY_GATE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp_r\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mFINAL_THRESHOLD\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3659846497.py\u001b[0m in \u001b[0;36mroberta_prob_one\u001b[0;34m(ctx, rep, max_length)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m   1189\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    796\u001b[0m                 \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    799\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2540\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2542\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected XLA tensor. Got: torch.FloatTensor"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "test_df = pd.DataFrame(ds[\"test\"])\n",
        "test_df.to_csv(\"test_set.csv\", index=False)\n",
        "\n",
        "print(\"Saved test set with\", len(test_df), \"rows\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jR1pg1ktZJv",
        "outputId": "4836b68f-503c-473b-d9d3-d58454d3bf56"
      },
      "id": "7jR1pg1ktZJv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved test set with 20500 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "os.listdir()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YsGtfy0uYQs",
        "outputId": "447b2fb4-cfa6-4658-e57b-04b850e5590d"
      },
      "id": "6YsGtfy0uYQs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'sarcasm_roberta_large_package',\n",
              " 'test_set.csv',\n",
              " 'sarcasm_roberta_large_package.zip',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find . -maxdepth 2 -type f | grep .ipynb\n"
      ],
      "metadata": {
        "id": "dzuh1RCRut0F"
      },
      "id": "dzuh1RCRut0F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import json, os, torch\n",
        "\n",
        "SAVE_DIR = \"/content/sarcasm_roberta_large_package\"  # or wherever you saved it\n",
        "p = \"/content/drive/MyDrive/model.safetensors\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(SAVE_DIR)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(p).to(device)\n",
        "model.eval()\n",
        "\n",
        "with open(os.path.join(SAVE_DIR, \"ensemble_config.json\"), \"r\") as f:\n",
        "    cfg = json.load(f)\n",
        "\n",
        "FINAL_THRESHOLD = cfg[\"final_threshold\"]\n",
        "MAX_LENGTH = cfg[\"max_length\"]\n",
        "KEEP_LAST_TURNS = cfg[\"keep_last_turns\"]\n",
        "\n",
        "UNCERTAINTY_GATE = cfg[\"uncertainty_gate\"]\n",
        "LLM_MODEL = cfg[\"llm_model\"]\n",
        "LLM_CONF_GATE = cfg[\"llm_conf_gate\"]\n",
        "W_ROBERTA_CONF = cfg[\"weights\"][\"roberta_conf\"]\n",
        "W_LLM_CONF = cfg[\"weights\"][\"llm_conf\"]\n",
        "W_ROBERTA_BASE = cfg[\"weights\"][\"roberta_base\"]\n",
        "W_LLM_BASE = cfg[\"weights\"][\"llm_base\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "ZLr14kP9jixu",
        "outputId": "d7e5ec4e-a5d1-4212-f888-4319bcfe77e6"
      },
      "id": "ZLr14kP9jixu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HFValidationError",
          "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/model.safetensors'. Use `repo_type` argument if needed.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    480\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/model.safetensors'. Use `repo_type` argument if needed.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3961963504.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    509\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \"\"\"\n\u001b[0;32m--> 322\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;31m# Now we try to recover if we can find all files correctly in the cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         resolved_files = [\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0m_get_cache_file_to_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfull_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         ]\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36m_get_cache_file_to_return\u001b[0;34m(path_or_repo_id, full_filename, cache_dir, revision, repo_type)\u001b[0m\n\u001b[1;32m    141\u001b[0m ):\n\u001b[1;32m    142\u001b[0m     \u001b[0;31m# We try to see if we have a cached version (not up to date):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     resolved_file = try_to_load_from_cache(\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         ):\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"token\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;34mf\" '{repo_id}'. Use `repo_type` argument if needed.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/model.safetensors'. Use `repo_type` argument if needed."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_ensemble_on_test(N=200)"
      ],
      "metadata": {
        "id": "hFU04mlAsbTR"
      },
      "id": "hFU04mlAsbTR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "p = \"/content/drive/MyDrive/model.safetensors\"\n",
        "print(\"Size (bytes):\", os.path.getsize(p))\n",
        "print(\"Size (GB):\", os.path.getsize(p)/1024**3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PRowObb2DrD",
        "outputId": "16d3be91-e65a-4bb0-e14f-b4988c9a161f"
      },
      "id": "_PRowObb2DrD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size (bytes): 1421495416\n",
            "Size (GB): 1.3238707706332207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run saved model in one go"
      ],
      "metadata": {
        "id": "EBOuIkTsr9M8"
      },
      "id": "EBOuIkTsr9M8"
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, hashlib, time\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from openai import OpenAI\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/sarcasm_roberta_large_package\"  # <-- use full path\n",
        "p = \"/content/drive/MyDrive\"\n",
        "\n",
        "print(\"Exists?\", os.path.exists(SAVE_DIR))\n",
        "print(\"Files:\", os.listdir(SAVE_DIR)[:10])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(SAVE_DIR, local_files_only=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(SAVE_DIR, local_files_only=True).to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Loaded local model ✅\")\n",
        "\n",
        "\n",
        "# ====== 3) Load ensemble config ======\n",
        "with open(os.path.join(SAVE_DIR, \"ensemble_config.json\"), \"r\") as f:\n",
        "    cfg = json.load(f)\n",
        "\n",
        "MAX_LENGTH       = int(cfg[\"max_length\"])\n",
        "KEEP_LAST_TURNS  = int(cfg[\"keep_last_turns\"])\n",
        "FINAL_THRESHOLD  = float(cfg[\"final_threshold\"])\n",
        "\n",
        "UNCERTAINTY_GATE = float(cfg[\"uncertainty_gate\"])\n",
        "LLM_MODEL        = cfg[\"llm_model\"]\n",
        "LLM_CONF_GATE    = float(cfg[\"llm_conf_gate\"])\n",
        "\n",
        "W_ROBERTA_CONF   = float(cfg[\"weights\"][\"roberta_conf\"])\n",
        "W_LLM_CONF       = float(cfg[\"weights\"][\"llm_conf\"])\n",
        "W_ROBERTA_BASE   = float(cfg[\"weights\"][\"roberta_base\"])\n",
        "W_LLM_BASE       = float(cfg[\"weights\"][\"llm_base\"])\n",
        "\n",
        "USE_OPENAI_ENSEMBLE = True\n",
        "\n",
        "UNCERTAINTY_GATE = 0.08\n",
        "LLM_MODEL = \"gpt-5.2\"\n",
        "LLM_CONF_GATE = 0.88\n",
        "W_ROBERTA_CONF, W_LLM_CONF = 0.75, 0.25\n",
        "W_ROBERTA_BASE, W_LLM_BASE = 0.90, 0.10\n",
        "FINAL_THRESHOLD = 0.56\n",
        "\n",
        "print(\"Loaded config:\", {\n",
        "    \"FINAL_THRESHOLD\": FINAL_THRESHOLD,\n",
        "    \"UNCERTAINTY_GATE\": UNCERTAINTY_GATE,\n",
        "    \"LLM_MODEL\": LLM_MODEL\n",
        "})\n",
        "\n",
        "# ====== 4) Load optional LLM cache ======\n",
        "cache_path = os.path.join(SAVE_DIR, \"llm_cache.json\")\n",
        "_llm_cache = json.load(open(cache_path)) if os.path.exists(cache_path) else {}\n",
        "print(\"LLM cache entries:\", len(_llm_cache))\n",
        "\n",
        "# ====== 5) Formatter ======\n",
        "def format_input(context_text: str, reply_text: str, keep_last_turns: int = KEEP_LAST_TURNS) -> str:\n",
        "    ctx = (context_text or \"\").strip()\n",
        "    rep = (reply_text or \"\").strip()\n",
        "    turns = [t.strip() for t in ctx.split(\"||\") if t.strip()]\n",
        "    turns = turns[-keep_last_turns:]\n",
        "    if not turns:\n",
        "        ctx_block = \"[NO_CONTEXT]\"\n",
        "    else:\n",
        "        ctx_block = \"\\n\".join([f\"[TURN-{len(turns)-i}] {t}\" for i, t in enumerate(turns)])\n",
        "    return f\"{ctx_block}\\n[REPLY] {rep}\"\n",
        "\n",
        "# ====== 6) RoBERTa prob ======\n",
        "def roberta_prob_one(context_text: str, reply_text: str) -> float:\n",
        "    text = format_input(context_text, reply_text)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        p = torch.softmax(logits, dim=-1)[0, 1].item()  # prob(sarcasm)\n",
        "    return float(p)\n",
        "\n",
        "# ====== 7) OpenAI judge (with caching + safe fallback) ======\n",
        "# Make sure OPENAI_API_KEY is set in your environment/secrets\n",
        "client = OpenAI()\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"You are a careful sarcasm detector for social media conversations.\\n\"\n",
        "    \"Sarcasm means intended meaning differs from literal meaning.\\n\"\n",
        "    \"Be conservative. Output ONLY strict JSON:\\n\"\n",
        "    '{\"label\":\"SARCASTIC\"|\"NOT_SARCASTIC\",\"confidence\":0-1}.'\n",
        ")\n",
        "\n",
        "def _cache_key(ctx: str, rep: str) -> str:\n",
        "    s = format_input(ctx, rep)\n",
        "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def llm_judge(ctx: str, rep: str, model_name: str = LLM_MODEL):\n",
        "    k = _cache_key(ctx, rep)\n",
        "    if k in _llm_cache:\n",
        "        return _llm_cache[k]  # (label_int, conf_float)\n",
        "\n",
        "    prompt = (\n",
        "        \"Decide whether the [REPLY] is sarcastic given the context.\\n\\n\"\n",
        "        f\"{format_input(ctx, rep)}\\n\\n\"\n",
        "        \"Return JSON only.\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        resp = client.responses.create(\n",
        "            model=model_name,\n",
        "            input=[\n",
        "                {\"role\": \"developer\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "        )\n",
        "        text = (resp.output_text or \"\").strip()\n",
        "        m = re.search(r\"\\{.*\\}\", text, flags=re.S)\n",
        "        if not m:\n",
        "            out = (0, 0.5)\n",
        "        else:\n",
        "            obj = json.loads(m.group(0))\n",
        "            lab = str(obj.get(\"label\", \"\")).upper()\n",
        "            conf = float(obj.get(\"confidence\", 0.5))\n",
        "            conf = max(0.0, min(1.0, conf))\n",
        "            out = (1 if lab == \"SARCASTIC\" else 0, conf)\n",
        "    except Exception:\n",
        "        # quota errors etc -> safe fallback\n",
        "        out = (0, 0.5)\n",
        "\n",
        "    _llm_cache[k] = out\n",
        "    # throttle a bit\n",
        "    time.sleep(0.2)\n",
        "    return out\n",
        "\n",
        "# ====== 8) Ensemble predictor ======\n",
        "def ensemble_predict_one(ctx: str, rep: str):\n",
        "    p_r = roberta_prob_one(ctx, rep)\n",
        "\n",
        "    # If RoBERTa is confident enough, don't call OpenAI\n",
        "    if abs(p_r - 0.5) >= UNCERTAINTY_GATE:\n",
        "        pred = 1 if p_r >= FINAL_THRESHOLD else 0\n",
        "        return pred, {\"used_llm\": False, \"p_roberta\": p_r, \"p_final\": p_r}\n",
        "\n",
        "    llm_label, llm_conf = llm_judge(ctx, rep)\n",
        "    # Convert LLM label+confidence into prob(sarcasm)\n",
        "    p_l = llm_conf if llm_label == 1 else (1.0 - llm_conf)\n",
        "\n",
        "    # Weighting: trust LLM more only when it's confident\n",
        "    if llm_conf >= LLM_CONF_GATE:\n",
        "        w_r, w_l = W_ROBERTA_CONF, W_LLM_CONF\n",
        "    else:\n",
        "        w_r, w_l = W_ROBERTA_BASE, W_LLM_BASE\n",
        "\n",
        "    p_final = w_r * p_r + w_l * p_l\n",
        "    pred = 1 if p_final >= FINAL_THRESHOLD else 0\n",
        "    return pred, {\n",
        "        \"used_llm\": True,\n",
        "        \"p_roberta\": p_r,\n",
        "        \"p_llm\": p_l,\n",
        "        \"llm_conf\": llm_conf,\n",
        "        \"p_final\": p_final,\n",
        "    }\n",
        "\n",
        "# ====== 9) Try it ======\n",
        "ctx = \"Oh, what is that are you hurt?\"\n",
        "rep = \"No /s\"\n",
        "pred, dbg = ensemble_predict_one(ctx, rep)\n",
        "print(\"Pred:\", \"sarcasm\" if pred==1 else \"not_sarcasm\")\n",
        "print(\"Debug:\", dbg)\n",
        "\n",
        "# ====== 10) Save updated cache back to disk (optional) ======\n",
        "with open(os.path.join(SAVE_DIR, \"llm_cache.json\"), \"w\") as f:\n",
        "    json.dump(_llm_cache, f)\n",
        "print(\"Cache saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-j-Aqw_r42X",
        "outputId": "5e877e21-3d5c-4243-84c6-38ba68ae31ce"
      },
      "id": "9-j-Aqw_r42X",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exists? True\n",
            "Files: ['model.safetensors', 'vocab.json', 'tokenizer_config.json', 'tokenizer.json', 'special_tokens_map.json', 'merges.txt', 'config.json', 'llm_cache.json', 'ensemble_config.json']\n",
            "Device: cpu\n",
            "Loaded local model ✅\n",
            "Loaded config: {'FINAL_THRESHOLD': 0.56, 'UNCERTAINTY_GATE': 0.08, 'LLM_MODEL': 'gpt-5.2'}\n",
            "LLM cache entries: 0\n",
            "Pred: not_sarcasm\n",
            "Debug: {'used_llm': False, 'p_roberta': 0.043070364743471146, 'p_final': 0.043070364743471146}\n",
            "Cache saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "def eval_ensemble(ds_test, N=300):\n",
        "    y_true, y_pred = [], []\n",
        "    used = 0\n",
        "    for i in range(min(N, len(ds_test))):\n",
        "        ex = ds_test[i]\n",
        "        pred, dbg = ensemble_predict_one(ex[\"context_text\"], ex[\"reply_text\"])\n",
        "        y_true.append(int(ex[\"label\"]))\n",
        "        y_pred.append(int(pred))\n",
        "        used += int(dbg.get(\"used_llm\", False))\n",
        "    print(\"N:\", len(y_true), \"| LLM used:\", used, f\"({100*used/len(y_true):.1f}%)\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred, target_names=[\"not_sarcasm\",\"sarcasm\"]))\n",
        "\n",
        "eval_ensemble(ds[\"test\"], N=600)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "C_1-rr-JsAVF",
        "outputId": "32f940f1-1148-4751-c467-0c9ec403b89d"
      },
      "id": "C_1-rr-JsAVF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ds' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3860071943.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"not_sarcasm\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"sarcasm\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0meval_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "def eval_ensemble_df(test_df, N=500):\n",
        "    y_true, y_pred = [], []\n",
        "    used = 0\n",
        "\n",
        "    for i in range(min(N, len(test_df))):\n",
        "        row = test_df.iloc[i]\n",
        "        pred, dbg = ensemble_predict_one(row[\"context_text\"], row[\"reply_text\"])\n",
        "        y_true.append(int(row[\"label\"]))\n",
        "        y_pred.append(int(pred))\n",
        "        if dbg.get(\"used_llm\"):\n",
        "            used += 1\n",
        "\n",
        "    print(\"Evaluated:\", len(y_true))\n",
        "    print(\"LLM used for:\", used, f\"({100*used/len(y_true):.1f}%)\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred, target_names=[\"not_sarcasm\",\"sarcasm\"]))\n"
      ],
      "metadata": {
        "id": "YDdFDEplyBiY"
      },
      "id": "YDdFDEplyBiY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/test_set.csv\")\n",
        "print(test_df.shape)\n",
        "test_df.head()\n",
        "\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Load TweetEval Irony test split\n",
        "external = load_dataset(\"cardiffnlp/tweet_eval\", \"irony\")[\"test\"]\n",
        "\n",
        "# Convert to your standard dataframe format\n",
        "df_external2 = pd.DataFrame({\n",
        "    \"reply_text\": external[\"text\"],\n",
        "    \"context_text\": [\"\"] * len(external),   # no parent context in this dataset\n",
        "    \"label\": [int(x) for x in external[\"label\"]],\n",
        "})\n",
        "\n",
        "print(\"External2 rows:\", len(df_external2))\n",
        "df_external2.head()\n",
        "\n",
        "\n",
        "eval_ensemble_df(df_external2, N=700)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEDBqYnryCXh",
        "outputId": "307c0706-2434-4de8-8c5e-7732c3b94d9f"
      },
      "id": "XEDBqYnryCXh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20500, 3)\n",
            "External2 rows: 784\n",
            "Evaluated: 700\n",
            "LLM used for: 73 (10.4%)\n",
            "[[355  72]\n",
            " [111 162]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " not_sarcasm       0.76      0.83      0.80       427\n",
            "     sarcasm       0.69      0.59      0.64       273\n",
            "\n",
            "    accuracy                           0.74       700\n",
            "   macro avg       0.73      0.71      0.72       700\n",
            "weighted avg       0.73      0.74      0.73       700\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "i3n1pJFxr7ol"
      },
      "id": "i3n1pJFxr7ol"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "29a74b5cf4a943e2a3f66ad26626e31a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a1db0df4383e47d0960bce2f8f364e42",
              "IPY_MODEL_718590f69fba4b3bb6f86725ed0121cb",
              "IPY_MODEL_fb00f115af9c4894b49f7279aa878bcc"
            ],
            "layout": "IPY_MODEL_2a29b7ed9b76480a9aca5c5cc9ea63df"
          }
        },
        "a1db0df4383e47d0960bce2f8f364e42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a199e5f48e474b8f858dd7a2ffb3589e",
            "placeholder": "​",
            "style": "IPY_MODEL_72e88a99df5147cbb3d0999051bbd477",
            "value": "Map: 100%"
          }
        },
        "718590f69fba4b3bb6f86725ed0121cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb6feeea3a374d9c8e3b4350289fcd30",
            "max": 164000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce62753d0939461b8630aa1122037986",
            "value": 164000
          }
        },
        "fb00f115af9c4894b49f7279aa878bcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e636a54762e47f4a433741a2d585b25",
            "placeholder": "​",
            "style": "IPY_MODEL_58be90f3622c4ec38fdb5122e6d164a1",
            "value": " 164000/164000 [00:23&lt;00:00, 7093.76 examples/s]"
          }
        },
        "2a29b7ed9b76480a9aca5c5cc9ea63df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a199e5f48e474b8f858dd7a2ffb3589e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72e88a99df5147cbb3d0999051bbd477": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb6feeea3a374d9c8e3b4350289fcd30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce62753d0939461b8630aa1122037986": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e636a54762e47f4a433741a2d585b25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58be90f3622c4ec38fdb5122e6d164a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c2e2f4f5d4f4bf6bf7fcfa83f627b67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02c02363d5594f6ba67cb2f3480a66b3",
              "IPY_MODEL_02e1bf192bbb4f3ca3f42a9579f579fb",
              "IPY_MODEL_0486272d5102468a85af918e403632bd"
            ],
            "layout": "IPY_MODEL_0c36dd011d5b4d6b897f627e2957ac05"
          }
        },
        "02c02363d5594f6ba67cb2f3480a66b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0aca254b80424a279738375857e84573",
            "placeholder": "​",
            "style": "IPY_MODEL_b2217ae0bff940ebb15df26fc8d269e1",
            "value": "Map: 100%"
          }
        },
        "02e1bf192bbb4f3ca3f42a9579f579fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dffdfb6d829403ab81cbb93fb3d690d",
            "max": 20500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f705b5e2bde4372bc1e754db513c352",
            "value": 20500
          }
        },
        "0486272d5102468a85af918e403632bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4cacdda5b0c4fe097264f5dade3b3bd",
            "placeholder": "​",
            "style": "IPY_MODEL_ac5513a977094b559dd64e7639cc5722",
            "value": " 20500/20500 [00:03&lt;00:00, 6394.31 examples/s]"
          }
        },
        "0c36dd011d5b4d6b897f627e2957ac05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0aca254b80424a279738375857e84573": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2217ae0bff940ebb15df26fc8d269e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8dffdfb6d829403ab81cbb93fb3d690d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f705b5e2bde4372bc1e754db513c352": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4cacdda5b0c4fe097264f5dade3b3bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac5513a977094b559dd64e7639cc5722": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a82a04917e7c4a3297caa00d6b22d4d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_45ad34e9c62f4ce9ba8610bc49c8f116",
              "IPY_MODEL_c9702b7fc4da41aab8495460ba99ecff",
              "IPY_MODEL_981e3a932c5d43fcb0e405f0bfd48269"
            ],
            "layout": "IPY_MODEL_a6cd312e484b470883f97bfbcb3e3f4b"
          }
        },
        "45ad34e9c62f4ce9ba8610bc49c8f116": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2ee315de85d4d7e8b77d47011bad217",
            "placeholder": "​",
            "style": "IPY_MODEL_adcdb06504bd4a7d8ab5a490deaec39b",
            "value": "Map: 100%"
          }
        },
        "c9702b7fc4da41aab8495460ba99ecff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e1e8b0d67d148659c2a569f56c5f21f",
            "max": 20500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b742a3ea51d4fd8bd5bbff4bf10e024",
            "value": 20500
          }
        },
        "981e3a932c5d43fcb0e405f0bfd48269": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29e57d88ab2c45c7ae5626393d989c76",
            "placeholder": "​",
            "style": "IPY_MODEL_9a7047e0d2494f779125bca25395dfe0",
            "value": " 20500/20500 [00:02&lt;00:00, 7241.40 examples/s]"
          }
        },
        "a6cd312e484b470883f97bfbcb3e3f4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2ee315de85d4d7e8b77d47011bad217": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adcdb06504bd4a7d8ab5a490deaec39b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e1e8b0d67d148659c2a569f56c5f21f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b742a3ea51d4fd8bd5bbff4bf10e024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29e57d88ab2c45c7ae5626393d989c76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a7047e0d2494f779125bca25395dfe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0689a9dd7bef4a72b559bb15848264bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d1d7c29df2364a0faebd23808f96febe",
              "IPY_MODEL_b695c15fd5cd44348d21194642adeaef",
              "IPY_MODEL_f7a2275284b8458e98767927fd22af13"
            ],
            "layout": "IPY_MODEL_385426cb9c3c4dde9f9d963a59a64e04"
          }
        },
        "d1d7c29df2364a0faebd23808f96febe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a330624c21b54f6cbac5562ab0b98dd0",
            "placeholder": "​",
            "style": "IPY_MODEL_fe070bff88404165b975a0d4c699f714",
            "value": "Casting the dataset: 100%"
          }
        },
        "b695c15fd5cd44348d21194642adeaef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13fa662e9b72429f88f2cb7dae08220a",
            "max": 164000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1156d0f9892a4a9f998632d69bdae418",
            "value": 164000
          }
        },
        "f7a2275284b8458e98767927fd22af13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_599fb3cc9911489bbd41ac3dcea7b2a0",
            "placeholder": "​",
            "style": "IPY_MODEL_1a63484620bc41a3b553080d91bb6a79",
            "value": " 164000/164000 [00:00&lt;00:00, 1063840.21 examples/s]"
          }
        },
        "385426cb9c3c4dde9f9d963a59a64e04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a330624c21b54f6cbac5562ab0b98dd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe070bff88404165b975a0d4c699f714": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13fa662e9b72429f88f2cb7dae08220a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1156d0f9892a4a9f998632d69bdae418": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "599fb3cc9911489bbd41ac3dcea7b2a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a63484620bc41a3b553080d91bb6a79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c02d7672ee0487bb8eb6616c37f4942": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ea6b02761b54de79ca5bcc637829621",
              "IPY_MODEL_8648d239b8954a7e90daa273d737453c",
              "IPY_MODEL_7c1d530b74c74675a4e7b0c1d9a7f061"
            ],
            "layout": "IPY_MODEL_dc0a1b083841413883a8cfea1bffd480"
          }
        },
        "8ea6b02761b54de79ca5bcc637829621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31dbb68567ab484abd42c163852e1dcb",
            "placeholder": "​",
            "style": "IPY_MODEL_52e2098aa973471fa86ba19520d1a6b3",
            "value": "Casting the dataset: 100%"
          }
        },
        "8648d239b8954a7e90daa273d737453c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_954420bfccbd4e10ad8ed9d7a07052ea",
            "max": 20500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca3d01a1df6f421e899a5bb55601921e",
            "value": 20500
          }
        },
        "7c1d530b74c74675a4e7b0c1d9a7f061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c62ccd27f3d48888ee55b8a487c2e3c",
            "placeholder": "​",
            "style": "IPY_MODEL_257f859596e7477dbba593c8911852ff",
            "value": " 20500/20500 [00:00&lt;00:00, 742559.84 examples/s]"
          }
        },
        "dc0a1b083841413883a8cfea1bffd480": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31dbb68567ab484abd42c163852e1dcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52e2098aa973471fa86ba19520d1a6b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "954420bfccbd4e10ad8ed9d7a07052ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca3d01a1df6f421e899a5bb55601921e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c62ccd27f3d48888ee55b8a487c2e3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "257f859596e7477dbba593c8911852ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4430d7f2bd2944aeab95f0341124ff00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ec1e46670e9449a8f05bbc392f02189",
              "IPY_MODEL_71d9fa8a568e49ac9446060802a88ec6",
              "IPY_MODEL_4f69efc2ee74474cba1fc651e50f954e"
            ],
            "layout": "IPY_MODEL_38ed447565db483eb4d6d62cfee949ca"
          }
        },
        "3ec1e46670e9449a8f05bbc392f02189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ea73ed59edf4eeea34b64501b1b27e3",
            "placeholder": "​",
            "style": "IPY_MODEL_8e1373903c4146abba2a25eff9a708a8",
            "value": "Casting the dataset: 100%"
          }
        },
        "71d9fa8a568e49ac9446060802a88ec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9284a59e9ddb45eca469fac4ab621070",
            "max": 20500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_22714caa36de49acac83871d930b2a98",
            "value": 20500
          }
        },
        "4f69efc2ee74474cba1fc651e50f954e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6d6653968f743f9a32ad0b8876f7cc6",
            "placeholder": "​",
            "style": "IPY_MODEL_448d957ed0144129a8cee583376fe321",
            "value": " 20500/20500 [00:00&lt;00:00, 335883.81 examples/s]"
          }
        },
        "38ed447565db483eb4d6d62cfee949ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ea73ed59edf4eeea34b64501b1b27e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e1373903c4146abba2a25eff9a708a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9284a59e9ddb45eca469fac4ab621070": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22714caa36de49acac83871d930b2a98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6d6653968f743f9a32ad0b8876f7cc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "448d957ed0144129a8cee583376fe321": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}